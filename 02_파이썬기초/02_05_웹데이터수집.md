# 02-05. ì›¹ë°ì´í„° ìˆ˜ì§‘

## ğŸ“‹ ëª©ì°¨

- [1. ì›¹ë°ì´í„° ìˆ˜ì§‘ ê°œìš”](#1-ì›¹ë°ì´í„°-ìˆ˜ì§‘-ê°œìš”)
- [2. requests ë¼ì´ë¸ŒëŸ¬ë¦¬](#2-requests-ë¼ì´ë¸ŒëŸ¬ë¦¬)
- [3. BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±](#3-beautifulsoupìœ¼ë¡œ-html-íŒŒì‹±)
- [4. Seleniumìœ¼ë¡œ ë™ì  ì›¹í˜ì´ì§€ ì²˜ë¦¬](#4-seleniumìœ¼ë¡œ-ë™ì -ì›¹í˜ì´ì§€-ì²˜ë¦¬)
- [5. ë°ì´í„° ì €ì¥](#5-ë°ì´í„°-ì €ì¥)
- [6. ì‹¤ì „ ì˜ˆì œ](#6-ì‹¤ì „-ì˜ˆì œ)
- [7. ì£¼ì˜ì‚¬í•­ê³¼ ëª¨ë²” ì‚¬ë¡€](#7-ì£¼ì˜ì‚¬í•­ê³¼-ëª¨ë²”-ì‚¬ë¡€)
- [8. ìš”ì•½](#8-ìš”ì•½)

---

## 1. ì›¹ë°ì´í„° ìˆ˜ì§‘ ê°œìš”

### 1.1 ì›¹ ìŠ¤í¬ë˜í•‘ì´ë€?

**ì›¹ ìŠ¤í¬ë˜í•‘(Web Scraping)**ì€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

**ì£¼ìš” ìš©ë„:**
- ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘
- ìƒí’ˆ ê°€ê²© ëª¨ë‹ˆí„°ë§
- ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘
- ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„° ë¶„ì„
- ì—°êµ¬ìš© ë°ì´í„° ìˆ˜ì§‘

### 1.2 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# ì„¤ì¹˜ ëª…ë ¹ì–´
# pip install requests beautifulsoup4 selenium pandas lxml

import requests          # HTTP ìš”ì²­
from bs4 import BeautifulSoup  # HTML íŒŒì‹±
from selenium import webdriver  # ë™ì  ì›¹í˜ì´ì§€ ì²˜ë¦¬
import pandas as pd     # ë°ì´í„° ì²˜ë¦¬
import time             # ëŒ€ê¸° ì‹œê°„
```

> âš ï¸ **Warning:** ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ í•´ë‹¹ ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ê³¼ robots.txtë¥¼ í™•ì¸í•˜ê³ , ì„œë²„ì— ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.

---

## 2. requests ë¼ì´ë¸ŒëŸ¬ë¦¬

### 2.1 ê¸°ë³¸ GET ìš”ì²­

```python
import requests

# ê¸°ë³¸ ìš”ì²­
response = requests.get("https://www.example.com")

# ìƒíƒœ ì½”ë“œ í™•ì¸
print(response.status_code)  # 200 (ì„±ê³µ)

# ì‘ë‹µ ë‚´ìš©
print(response.text)  # HTML ë‚´ìš©
print(response.content)  # ë°”ì´ë„ˆë¦¬ ë°ì´í„°
```

### 2.2 í—¤ë” ì„¤ì •

```python
import requests

# User-Agent ì„¤ì • (ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

response = requests.get("https://www.example.com", headers=headers)
```

### 2.3 íŒŒë¼ë¯¸í„° ì „ë‹¬

```python
import requests

# URL íŒŒë¼ë¯¸í„°
params = {
    "q": "python",
    "page": 1
}

response = requests.get("https://www.example.com/search", params=params)
print(response.url)  # ì‹¤ì œ ìš”ì²­ URL í™•ì¸
```

### 2.4 ì—ëŸ¬ ì²˜ë¦¬

```python
import requests

try:
    response = requests.get("https://www.example.com", timeout=5)
    response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
    print("ìš”ì²­ ì„±ê³µ")
except requests.exceptions.RequestException as e:
    print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
```

---

## 3. BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±

### 3.1 ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import requests
from bs4 import BeautifulSoup

# ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
url = "https://www.example.com"
response = requests.get(url)
html = response.text

# BeautifulSoup ê°ì²´ ìƒì„±
soup = BeautifulSoup(html, "html.parser")

# HTML êµ¬ì¡° í™•ì¸
print(soup.prettify())
```

### 3.2 íƒœê·¸ ì°¾ê¸°

```python
from bs4 import BeautifulSoup

html = """
<html>
<body>
    <h1>ì œëª©</h1>
    <p class="content">ë‚´ìš© 1</p>
    <p class="content">ë‚´ìš© 2</p>
    <a href="https://example.com">ë§í¬</a>
</body>
</html>
"""

soup = BeautifulSoup(html, "html.parser")

# íƒœê·¸ë¡œ ì°¾ê¸°
title = soup.find("h1")
print(title.text)  # ì œëª©

# ëª¨ë“  íƒœê·¸ ì°¾ê¸°
paragraphs = soup.find_all("p")
for p in paragraphs:
    print(p.text)

# í´ë˜ìŠ¤ë¡œ ì°¾ê¸°
contents = soup.find_all("p", class_="content")

# IDë¡œ ì°¾ê¸°
element = soup.find(id="my-id")

# ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
link = soup.find("a", href="https://example.com")
```

### 3.3 CSS ì„ íƒì ì‚¬ìš©

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, "html.parser")

# CSS ì„ íƒìë¡œ ì°¾ê¸°
title = soup.select_one("h1")
paragraphs = soup.select("p.content")
link = soup.select_one("a[href='https://example.com']")

# ì¤‘ì²© ì„ íƒ
items = soup.select("div.container > ul > li")
```

### 3.4 ë°ì´í„° ì¶”ì¶œ

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, "html.parser")

# í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = soup.find("p").text
text = soup.find("p").get_text()

# ì†ì„± ì¶”ì¶œ
link = soup.find("a")
href = link["href"]
href = link.get("href")

# HTML ì¶”ì¶œ
html_content = soup.find("div").prettify()
```

### 3.5 ì‹¤ì „ ì˜ˆì œ: ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘

```python
import requests
from bs4 import BeautifulSoup

def get_news_headlines(url):
    """ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    
    headlines = []
    # ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
    for article in soup.find_all("h2", class_="headline"):
        title = article.text.strip()
        link = article.find("a")["href"] if article.find("a") else ""
        headlines.append({"title": title, "link": link})
    
    return headlines

# ì‚¬ìš© ì˜ˆì‹œ
# headlines = get_news_headlines("https://news.example.com")
# for headline in headlines:
#     print(headline["title"])
```

---

## 4. Seleniumìœ¼ë¡œ ë™ì  ì›¹í˜ì´ì§€ ì²˜ë¦¬

### 4.1 Selenium ì„¤ì •

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Chrome ë“œë¼ì´ë²„ ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¸°ê¸°
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(options=options)
```

### 4.2 í˜ì´ì§€ ë¡œë“œ ë° ìš”ì†Œ ì°¾ê¸°

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()

# í˜ì´ì§€ ì—´ê¸°
driver.get("https://www.example.com")

# ìš”ì†Œ ì°¾ê¸°
element = driver.find_element(By.ID, "my-id")
element = driver.find_element(By.CLASS_NAME, "my-class")
element = driver.find_element(By.CSS_SELECTOR, "div.content")

# ì—¬ëŸ¬ ìš”ì†Œ ì°¾ê¸°
elements = driver.find_elements(By.TAG_NAME, "p")

# í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
text = element.text

# ì†ì„± ê°€ì ¸ì˜¤ê¸°
href = element.get_attribute("href")

driver.quit()
```

### 4.3 ëŒ€ê¸° ì²˜ë¦¬

```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ëª…ì‹œì  ëŒ€ê¸°
wait = WebDriverWait(driver, 10)
element = wait.until(EC.presence_of_element_located((By.ID, "my-id")))

# ì•”ì‹œì  ëŒ€ê¸°
driver.implicitly_wait(10)

# ì‹œê°„ ëŒ€ê¸°
import time
time.sleep(2)
```

### 4.4 ë™ì  ì½˜í…ì¸  ì²˜ë¦¬

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

driver = webdriver.Chrome()
driver.get("https://www.example.com")

# ìŠ¤í¬ë¡¤
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

# í´ë¦­
button = driver.find_element(By.ID, "load-more")
button.click()

# ì…ë ¥
search_box = driver.find_element(By.NAME, "q")
search_box.send_keys("python")
search_box.send_keys(Keys.RETURN)

driver.quit()
```

---

## 5. ë°ì´í„° ì €ì¥

### 5.1 CSV íŒŒì¼ë¡œ ì €ì¥

```python
import pandas as pd

# ë°ì´í„° ì¤€ë¹„
data = [
    {"ì´ë¦„": "í™ê¸¸ë™", "ë‚˜ì´": 25, "ë„ì‹œ": "ì„œìš¸"},
    {"ì´ë¦„": "ì´ì˜í¬", "ë‚˜ì´": 23, "ë„ì‹œ": "ë¶€ì‚°"},
    {"ì´ë¦„": "ê¹€ì² ìˆ˜", "ë‚˜ì´": 27, "ë„ì‹œ": "ëŒ€êµ¬"}
]

# DataFrame ìƒì„±
df = pd.DataFrame(data)

# CSVë¡œ ì €ì¥
df.to_csv("data.csv", index=False, encoding="utf-8-sig")

# CSV ì½ê¸°
df = pd.read_csv("data.csv", encoding="utf-8-sig")
print(df)
```

### 5.2 JSON íŒŒì¼ë¡œ ì €ì¥

```python
import json

# ë°ì´í„° ì¤€ë¹„
data = [
    {"ì´ë¦„": "í™ê¸¸ë™", "ë‚˜ì´": 25},
    {"ì´ë¦„": "ì´ì˜í¬", "ë‚˜ì´": 23}
]

# JSONìœ¼ë¡œ ì €ì¥
with open("data.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

# JSON ì½ê¸°
with open("data.json", "r", encoding="utf-8") as f:
    data = json.load(f)
```

### 5.3 Excel íŒŒì¼ë¡œ ì €ì¥

```python
import pandas as pd

df = pd.DataFrame(data)

# Excelë¡œ ì €ì¥
df.to_excel("data.xlsx", index=False, engine="openpyxl")

# Excel ì½ê¸°
df = pd.read_excel("data.xlsx", engine="openpyxl")
```

---

## 6. ì‹¤ì „ ì˜ˆì œ

### ì˜ˆì œ 1: ê°„ë‹¨í•œ ì›¹ ìŠ¤í¬ë˜í¼

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_website(url):
    """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    
    data = []
    # ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
    items = soup.find_all("div", class_="item")
    
    for item in items:
        title = item.find("h3").text.strip() if item.find("h3") else ""
        price = item.find("span", class_="price").text.strip() if item.find("span", class_="price") else ""
        data.append({"ì œëª©": title, "ê°€ê²©": price})
    
    return data

# ì‚¬ìš©
# url = "https://www.example.com/products"
# data = scrape_website(url)
# df = pd.DataFrame(data)
# df.to_csv("products.csv", index=False, encoding="utf-8-sig")
```

### ì˜ˆì œ 2: ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_multiple_pages(base_url, max_pages=5):
    """ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    all_data = []
    
    for page in range(1, max_pages + 1):
        url = f"{base_url}?page={page}"
        print(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
        items = soup.find_all("div", class_="item")
        for item in items:
            # ë°ì´í„° ì¶”ì¶œ ë¡œì§
            all_data.append({})  # ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´
        
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(1)
    
    return all_data

# ì‚¬ìš©
# data = scrape_multiple_pages("https://www.example.com/list", max_pages=10)
# df = pd.DataFrame(data)
# df.to_csv("all_data.csv", index=False, encoding="utf-8-sig")
```

### ì˜ˆì œ 3: ë™ì  ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time

def scrape_dynamic_page(url):
    """ë™ì  ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    
    driver = webdriver.Chrome(options=options)
    driver.get(url)
    
    # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "content")))
    
    data = []
    
    # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ (í•„ìš”í•œ ê²½ìš°)
    try:
        while True:
            more_button = driver.find_element(By.ID, "load-more")
            if more_button.is_displayed():
                more_button.click()
                time.sleep(2)
            else:
                break
    except:
        pass
    
    # ë°ì´í„° ì¶”ì¶œ
    items = driver.find_elements(By.CLASS_NAME, "item")
    for item in items:
        title = item.find_element(By.TAG_NAME, "h3").text
        # ì¶”ê°€ ë°ì´í„° ì¶”ì¶œ
        data.append({"ì œëª©": title})
    
    driver.quit()
    return data

# ì‚¬ìš©
# data = scrape_dynamic_page("https://www.example.com")
# df = pd.DataFrame(data)
# df.to_csv("dynamic_data.csv", index=False, encoding="utf-8-sig")
```

---

## 7. ì£¼ì˜ì‚¬í•­ê³¼ ëª¨ë²” ì‚¬ë¡€

### 7.1 ë²•ì  ë° ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­

```python
# 1. robots.txt í™•ì¸
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://www.example.com/robots.txt")
rp.read()

if rp.can_fetch("*", "https://www.example.com/page"):
    # ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥
    pass
else:
    # ìŠ¤í¬ë˜í•‘ ë¶ˆê°€
    print("ìŠ¤í¬ë˜í•‘ì´ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
```

### 7.2 ì„œë²„ ë¶€í•˜ ë°©ì§€

```python
import time
import random

# ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„
time.sleep(1)  # ê³ ì • ëŒ€ê¸°
time.sleep(random.uniform(1, 3))  # ëœë¤ ëŒ€ê¸°

# ìš”ì²­ ì œí•œ
max_requests = 100
request_count = 0

for url in urls:
    if request_count >= max_requests:
        break
    # ìš”ì²­ ì²˜ë¦¬
    request_count += 1
    time.sleep(1)
```

### 7.3 ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„

```python
import requests
import time

def fetch_with_retry(url, max_retries=3):
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ìš”ì²­"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                print(f"ì¬ì‹œë„ {attempt + 1}/{max_retries} ({wait_time}ì´ˆ ëŒ€ê¸°)")
                time.sleep(wait_time)
            else:
                print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
                return None
    return None
```

### 7.4 ëª¨ë²” ì‚¬ë¡€ ì²´í¬ë¦¬ìŠ¤íŠ¸

- âœ… robots.txt í™•ì¸
- âœ… User-Agent ì„¤ì •
- âœ… ì ì ˆí•œ ëŒ€ê¸° ì‹œê°„ ì„¤ì •
- âœ… ì—ëŸ¬ ì²˜ë¦¬ êµ¬í˜„
- âœ… ë°ì´í„° ê²€ì¦
- âœ… ìš”ì²­ ì œí•œ ì„¤ì •
- âœ… ì´ìš©ì•½ê´€ ì¤€ìˆ˜

---

## 8. ìš”ì•½

### í•µì‹¬ ì •ë¦¬

1. **requests**: HTTP ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
2. **BeautifulSoup**: HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ
3. **Selenium**: ë™ì  ì›¹í˜ì´ì§€ ì²˜ë¦¬
4. **ë°ì´í„° ì €ì¥**: CSV, JSON, Excel í˜•ì‹ìœ¼ë¡œ ì €ì¥
5. **ì—ëŸ¬ ì²˜ë¦¬**: ì¬ì‹œë„ ë¡œì§ ë° ì˜ˆì™¸ ì²˜ë¦¬
6. **ìœ¤ë¦¬ì  ìŠ¤í¬ë˜í•‘**: robots.txt í™•ì¸, ì„œë²„ ë¶€í•˜ ë°©ì§€

### AICE ì‹¤ì „ íŒ

- **ì„ íƒì**: CSS ì„ íƒìì™€ find ë©”ì„œë“œ ì¡°í•© í™œìš©
- **ëŒ€ê¸° ì‹œê°„**: ë™ì  ì½˜í…ì¸ ëŠ” ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„ í•„ìš”
- **ë°ì´í„° ê²€ì¦**: ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ìœ íš¨ì„± í™•ì¸
- **ì½”ë“œ ì¬ì‚¬ìš©**: í•¨ìˆ˜ë¡œ ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„± í–¥ìƒ

> ğŸ’¡ **Tip:** ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ë¯¼ê°í•©ë‹ˆë‹¤. ì •ê¸°ì ìœ¼ë¡œ ì½”ë“œë¥¼ ì ê²€í•˜ê³  ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] requestsë¡œ HTTP ìš”ì²­ ë³´ë‚´ê¸°
- [ ] BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
- [ ] CSS ì„ íƒìë¡œ ìš”ì†Œ ì°¾ê¸°
- [ ] Seleniumìœ¼ë¡œ ë™ì  ì›¹í˜ì´ì§€ ì²˜ë¦¬
- [ ] ë°ì´í„°ë¥¼ CSV/JSONìœ¼ë¡œ ì €ì¥
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
- [ ] robots.txt í™•ì¸ ë° ìœ¤ë¦¬ì  ìŠ¤í¬ë˜í•‘
- [ ] ì‹¤ìŠµ ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰ ì™„ë£Œ

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [requests ê³µì‹ ë¬¸ì„œ](https://requests.readthedocs.io/)
- [BeautifulSoup ê³µì‹ ë¬¸ì„œ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium ê³µì‹ ë¬¸ì„œ](https://www.selenium.dev/documentation/)
- [Pandas ê³µì‹ ë¬¸ì„œ](https://pandas.pydata.org/docs/)

---

**ì‘ì„±ì¼**: 2025-11-18  
**ë‹¤ìŒ í•™ìŠµ**: 03_ë°ì´í„°ë¶„ì„

